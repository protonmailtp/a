                                   ___                _   _           _     
  /\  /\__ _ _ __  _ __  _   _    / _ \_ __ __ _  ___| |_(_) ___ __ _| |___ 
 / /_/ / _` | '_ \| '_ \| | | |  / /_)/ '__/ _` |/ __| __| |/ __/ _` | / __|
/ __  / (_| | |_) | |_) | |_| | / ___/| | | (_| | (__| |_| | (_| (_| | \__ \
\/ /_/ \__,_| .__/| .__/ \__, | \/    |_|  \__,_|\___|\__|_|\___\__,_|_|___/
            |_|   |_|    |___/                                              
			@@@ Encoded By cmV4b3I and VjFBQ0s

1 uber (uber.csv)

#====

#Importing the required libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

#importing the dataset
df  = pd.read_csv("uber.csv")

##  1.	Pre-process the dataset.

df.head()

df.info() #To get the required information of the dataset

df.columns #TO get number of columns in the dataset

df = df.drop(['Unnamed: 0', 'key'], axis= 1) #To drop unnamed column as it isn't required

df.head()

df.shape #To get the total (Rows,Columns)

df.dtypes #To get the type of each column

df.info()


df.describe() #To get statistics of each columns 

### Filling Missing values

df.isnull().sum() 

df['dropoff_latitude'].fillna(value=df['dropoff_latitude'].mean(),inplace = True)
df['dropoff_longitude'].fillna(value=df['dropoff_longitude'].median(),inplace = True)

df.isnull().sum() 

df.dtypes

### Column pickup_datetime is in wrong format (Object). Convert it to DateTime Format

df.pickup_datetime = pd.to_datetime(df.pickup_datetime, errors='coerce') 

df.dtypes

### To segregate each time of date and time

df= df.assign(hour = df.pickup_datetime.dt.hour,
             day= df.pickup_datetime.dt.day,
             month = df.pickup_datetime.dt.month,
             year = df.pickup_datetime.dt.year,
             dayofweek = df.pickup_datetime.dt.dayofweek)

df.head()

# drop the column 'pickup_daetime' using drop()
# 'axis = 1' drops the specified column

df = df.drop('pickup_datetime',axis=1)


df.head()

df.dtypes

## Checking outliers and filling them 

df.plot(kind = "box",subplots = True,layout = (7,2),figsize=(15,20)) #Boxplot to check the outliers

#Using the InterQuartile Range to fill the values
def remove_outlier(df1 , col):
    Q1 = df1[col].quantile(0.25)
    Q3 = df1[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_whisker = Q1-1.5*IQR
    upper_whisker = Q3+1.5*IQR
    df[col] = np.clip(df1[col] , lower_whisker , upper_whisker)
    return df1

def treat_outliers_all(df1 , col_list):
    for c in col_list:
        df1 = remove_outlier(df , c)
    return df1

df = treat_outliers_all(df , df.iloc[: , 0::])

df.plot(kind = "box",subplots = True,layout = (7,2),figsize=(15,20)) #Boxplot shows that dataset is free from outliers

#pip install haversine
import haversine as hs  #Calculate the distance using Haversine to calculate the distance between to points. Can't use Eucladian as it is for flat surface.
travel_dist = []
for pos in range(len(df['pickup_longitude'])):
        long1,lati1,long2,lati2 = [df['pickup_longitude'][pos],df['pickup_latitude'][pos],df['dropoff_longitude'][pos],df['dropoff_latitude'][pos]]
        loc1=(lati1,long1)
        loc2=(lati2,long2)
        c = hs.haversine(loc1,loc2)
        travel_dist.append(c)
    
print(travel_dist)
df['dist_travel_km'] = travel_dist
df.head()

#Uber doesn't travel over 130 kms so minimize the distance 
df= df.loc[(df.dist_travel_km >= 1) | (df.dist_travel_km <= 130)]
print("Remaining observastions in the dataset:", df.shape)

#Finding inccorect latitude (Less than or greater than 90) and longitude (greater than or less than 180)
incorrect_coordinates = df.loc[(df.pickup_latitude > 90) |(df.pickup_latitude < -90) |
                                   (df.dropoff_latitude > 90) |(df.dropoff_latitude < -90) |
                                   (df.pickup_longitude > 180) |(df.pickup_longitude < -180) |
                                   (df.dropoff_longitude > 90) |(df.dropoff_longitude < -90)
                                    ]

df.drop(incorrect_coordinates, inplace = True, errors = 'ignore')

df.head()

df.isnull().sum()

sns.heatmap(df.isnull()) #Free for null values

corr = df.corr() #Function to find the correlation

corr

fig,axis = plt.subplots(figsize = (10,6))
sns.heatmap(df.corr(),annot = True) #Correlation Heatmap (Light values means highly correlated)

### Dividing the dataset into feature and target values 

x = df[['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count','hour','day','month','year','dayofweek','dist_travel_km']]

y = df['fare_amount']

### Dividing the dataset into training and testing dataset

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(x,y,test_size = 0.33)

### Linear Regression

from sklearn.linear_model import LinearRegression
regression = LinearRegression()

regression.fit(X_train,y_train)

regression.intercept_ #To find the linear intercept

regression.coef_ #To find the linear coeeficient

prediction = regression.predict(X_test) #To predict the target values

print(prediction)

y_test

### Metrics Evaluation using R2, Mean Squared Error, Root Mean Sqared Error

from sklearn.metrics import r2_score 

r2_score(y_test,prediction)

from sklearn.metrics import mean_squared_error

MSE = mean_squared_error(y_test,prediction)

MSE 

RMSE = np.sqrt(MSE)

RMSE

### Random Forest Regression

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100) #Here n_estimators means number of trees you want to build before making the prediction

rf.fit(X_train,y_train)

y_pred = rf.predict(X_test)

y_pred

### Metrics evaluatin for Random Forest

R2_Random = r2_score(y_test,y_pred)

R2_Random

MSE_Random = mean_squared_error(y_test,y_pred)

MSE_Random

RMSE_Random = np.sqrt(MSE_Random)

RMSE_Random

#====







2 email (emails.csv)

#====

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn import metrics

df=pd.read_csv('emails.csv')

df.head()

df.isnull().sum()

df.dropna(inplace = True)

df.drop(['Email No.'],axis=1,inplace=True)
X = df.drop(['Prediction'],axis = 1)
y = df['Prediction']

from sklearn.preprocessing import scale
X = scale(X)
# split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

##KNN classifier

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=7)
 
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)

print("Prediction",y_pred)

print("KNN accuracy = ",metrics.accuracy_score(y_test,y_pred))

print("Confusion matrix",metrics.confusion_matrix(y_test,y_pred))

## SVM classifier

# cost C = 1
model = SVC(C = 1)

# fit
model.fit(X_train, y_train)

# predict
y_pred = model.predict(X_test)

metrics.confusion_matrix(y_true=y_test, y_pred=y_pred)

print("SVM accuracy = ",metrics.accuracy_score(y_test,y_pred))

#====







3 churn neural network (Churn_Modelling.csv)

#====

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt #Importing the libraries

df = pd.read_csv("Churn_Modelling.csv")

# Preprocessing. 

df.head()

df.shape

df.describe()

df.isnull()

df.isnull().sum()

df.info()

df.dtypes

df.columns

df = df.drop(['RowNumber', 'Surname', 'CustomerId'], axis= 1) #Dropping the unnecessary columns 

df.head()

# Visualization

def visualization(x, y, xlabel):
    plt.figure(figsize=(10,5))
    plt.hist([x, y], color=['red', 'green'], label = ['exit', 'not_exit'])
    plt.xlabel(xlabel,fontsize=20)
    plt.ylabel("No. of customers", fontsize=20)
    plt.legend()

df_churn_exited = df[df['Exited']==1]['Tenure']
df_churn_not_exited = df[df['Exited']==0]['Tenure']

visualization(df_churn_exited, df_churn_not_exited, "Tenure")

df_churn_exited2 = df[df['Exited']==1]['Age']
df_churn_not_exited2 = df[df['Exited']==0]['Age']

visualization(df_churn_exited2, df_churn_not_exited2, "Age")

# Converting the Categorical Variables

X = df[['CreditScore','Gender','Age','Tenure','Balance','NumOfProducts','HasCrCard','IsActiveMember','EstimatedSalary']]
states = pd.get_dummies(df['Geography'],drop_first = True)
gender = pd.get_dummies(df['Gender'],drop_first = True)


df = pd.concat([df,gender,states], axis = 1)

# Splitting the training and testing Dataset

df.head()

X = df[['CreditScore','Age','Tenure','Balance','NumOfProducts','HasCrCard','IsActiveMember','EstimatedSalary','Male','Germany','Spain']]

y = df['Exited']

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.30)

# Normalizing the values with mean as 0 and Standard Deviation as 1

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

X_train  = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

X_train

X_test

# Building the Classifier Model using Keras 

from tensorflow import keras #Keras is the wrapper on the top of tenserflow
#Can use Tenserflow as well but won't be able to understand the errors initially. 

from keras.models import Sequential #To create sequential neural network
from keras.layers import Dense #To create hidden layers

classifier = Sequential()

#To add the layers
#Dense helps to contruct the neurons
#Input Dimension means we have 11 features 
# Units is to create the hidden layers
#Uniform helps to distribute the weight uniformly
classifier.add(Dense(activation = "relu",input_dim = 11,units = 6,kernel_initializer = "uniform")) 

classifier.add(Dense(activation = "relu",units = 6,kernel_initializer = "uniform"))   #Adding second hidden layers

classifier.add(Dense(activation = "sigmoid",units = 1,kernel_initializer = "uniform")) #Final neuron will be having siigmoid function

classifier.compile(optimizer="adam",loss = 'binary_crossentropy',metrics = ['accuracy']) #To compile the Artificial Neural Network. Ussed Binary crossentropy as we just have only two output

classifier.summary() #3 layers created. 6 neurons in 1st,6neurons in 2nd layer and 1 neuron in last

classifier.fit(X_train,y_train,batch_size=10,epochs=50) #Fitting the ANN to training dataset

y_pred =classifier.predict(X_test)
y_pred = (y_pred > 0.5) #Predicting the result

from sklearn.metrics import confusion_matrix,accuracy_score,classification_report

cm = confusion_matrix(y_test,y_pred)

cm

accuracy = accuracy_score(y_test,y_pred)

accuracy

plt.figure(figsize = (10,7))
sns.heatmap(cm,annot = True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

print(classification_report(y_test,y_pred))

#====







4 k-nearest neighbors diabetes (diabetes.csv)

#====

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn import metrics

df=pd.read_csv('diabetes.csv')

df.columns

Check for null values. If present remove null values from the dataset

df.isnull().sum()



Outcome is the label/target, other columns are features

X = df.drop('Outcome',axis = 1)
y = df['Outcome']

from sklearn.preprocessing import scale
X = scale(X)
# split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=7)
 
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)

print("Confusion matrix: ")
cs = metrics.confusion_matrix(y_test,y_pred)
print(cs)

print("Acccuracy ",metrics.accuracy_score(y_test,y_pred))

Classification error rate: proportion of instances misclassified over the whole set of instances.
Error rate is calculated as the total number of two incorrect predictions (FN + FP) divided by the total number of a dataset (examples in the dataset.

Also error_rate = 1- accuracy

total_misclassified = cs[0,1] + cs[1,0]
print(total_misclassified)
total_examples = cs[0,0]+cs[0,1]+cs[1,0]+cs[1,1]
print(total_examples)
print("Error rate",total_misclassified/total_examples)
print("Error rate ",1-metrics.accuracy_score(y_test,y_pred))

print("Precision score",metrics.precision_score(y_test,y_pred))

print("Recall score ",metrics.recall_score(y_test,y_pred))

print("Classification report ",metrics.classification_report(y_test,y_pred))

#====







5 k-means clustering (sales_data_sample.csv)

#====

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
#Importing the required libraries.

from sklearn.cluster import KMeans, k_means #For clustering
from sklearn.decomposition import PCA #Linear Dimensionality reduction.

df = pd.read_csv("sales_data_sample.csv") #Loading the dataset.

## Preprocessing 

df.head()

df.shape 

df.describe()

df.info()

df.isnull().sum()

df.dtypes

df_drop  = ['ADDRESSLINE1', 'ADDRESSLINE2', 'STATUS','POSTALCODE', 'CITY', 'TERRITORY', 'PHONE', 'STATE', 'CONTACTFIRSTNAME', 'CONTACTLASTNAME', 'CUSTOMERNAME', 'ORDERNUMBER']
df = df.drop(df_drop, axis=1) #Dropping the categorical uneccessary columns along with columns having null values. Can't fill the null values are there are alot of null values.

df.isnull().sum()

df.dtypes

# Checking the categorical columns.

df['COUNTRY'].unique()

df['PRODUCTLINE'].unique()

df['DEALSIZE'].unique()

productline = pd.get_dummies(df['PRODUCTLINE']) #Converting the categorical columns. 
Dealsize = pd.get_dummies(df['DEALSIZE'])

df = pd.concat([df,productline,Dealsize], axis = 1)

df_drop  = ['COUNTRY','PRODUCTLINE','DEALSIZE'] #Dropping Country too as there are alot of countries. 
df = df.drop(df_drop, axis=1)

df['PRODUCTCODE'] = pd.Categorical(df['PRODUCTCODE']).codes #Converting the datatype.

df.drop('ORDERDATE', axis=1, inplace=True) #Dropping the Orderdate as Month is already included.

df.dtypes #All the datatypes are converted into numeric

## Plotting the Elbow Plot to determine the number of clusters. 

distortions = [] # Within Cluster Sum of Squares from the centroid
K = range(1,10)
for k in K:
    kmeanModel = KMeans(n_clusters=k)
    kmeanModel.fit(df)
    distortions.append(kmeanModel.inertia_)   #Appeding the intertia to the Distortions 

plt.figure(figsize=(16,8))
plt.plot(K, distortions, 'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()

## As the number of k increases Inertia decreases. 
## Observations: A Elbow can be observed at 3 and after that the curve decreases gradually. 

X_train = df.values #Returns a numpy array.

X_train.shape

model = KMeans(n_clusters=3,random_state=2) #Number of cluster = 3
model = model.fit(X_train) #Fitting the values to create a model.
predictions = model.predict(X_train) #Predicting the cluster values (0,1,or 2)

unique,counts = np.unique(predictions,return_counts=True)

counts = counts.reshape(1,3)

counts_df = pd.DataFrame(counts,columns=['Cluster1','Cluster2','Cluster3'])

counts_df.head()

## Visualization 

pca = PCA(n_components=2) #Converting all the features into 2 columns to make it easy to visualize using Principal COmponent Analysis.

reduced_X = pd.DataFrame(pca.fit_transform(X_train),columns=['PCA1','PCA2']) #Creating a DataFrame.

reduced_X.head()

#Plotting the normal Scatter Plot
plt.figure(figsize=(14,10))
plt.scatter(reduced_X['PCA1'],reduced_X['PCA2'])

model.cluster_centers_ #Finding the centriods. (3 Centriods in total. Each Array contains a centroids for particular feature )

reduced_centers = pca.transform(model.cluster_centers_) #Transforming the centroids into 3 in x and y coordinates

reduced_centers

plt.figure(figsize=(14,10))
plt.scatter(reduced_X['PCA1'],reduced_X['PCA2'])
plt.scatter(reduced_centers[:,0],reduced_centers[:,1],color='black',marker='x',s=300) #Plotting the centriods

reduced_X['Clusters'] = predictions #Adding the Clusters to the reduced dataframe.

reduced_X.head()

#Plotting the clusters 
plt.figure(figsize=(14,10))
#                     taking the cluster number and first column           taking the same cluster number and second column      Assigning the color
plt.scatter(reduced_X[reduced_X['Clusters'] == 0].loc[:,'PCA1'],reduced_X[reduced_X['Clusters'] == 0].loc[:,'PCA2'],color='slateblue')
plt.scatter(reduced_X[reduced_X['Clusters'] == 1].loc[:,'PCA1'],reduced_X[reduced_X['Clusters'] == 1].loc[:,'PCA2'],color='springgreen')
plt.scatter(reduced_X[reduced_X['Clusters'] == 2].loc[:,'PCA1'],reduced_X[reduced_X['Clusters'] == 2].loc[:,'PCA2'],color='indigo')


plt.scatter(reduced_centers[:,0],reduced_centers[:,1],color='black',marker='x',s=300)

#====


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
theory time

1 uber theory

Linear Regression:
Linear regression is one of the easiest and most popular Machine Learning algorithms. It is a statistical method
that is used for predictive analysis. Linear regression makes predictions for continuous/real or numeric variables
such as sales, salary, age, product price, etc.
Linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent
(y) variables, hence called as linear regression. Since linear regression shows the linear relationship, which
means it finds how the value of the dependent variable is changing according to the value of the independent
variable.
The linear regression model provides a sloped straight line representing the relationship between the variables.
Mathematically, we can represent a linear regression as:
y= a0+a1x+ ε
Here,
Y= Dependent Variable (Target Variable)
X= Independent Variable (predictor Variable)
a0= intercept of the line (Gives an additional degree of freedom)
a1 = Linear regression coefficient (scale factor to each input value).
ε = random error

Random Forest:
Random Forest is a popular machine learning algorithm that belongs to the supervised learning technique. It can
be used for both Classification and Regression problems in ML. It is based on the concept of ensemble
learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the
performance of the model.
As the name suggests, "Random Forest is a classifier that contains a number of decision trees on various subsets
of the given dataset and takes the average to improve the predictive accuracy of that dataset." Instead of relying
on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of
predictions, and it predicts the final output.
The greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

Python Packages needed
● pandas
 Data Analytics
● numpy
 Numerical Computing
● matplotlib.pyplot
 Plotting graphs
● Sklearn
 Regression Classes

*********Code:********

Importing Libraries: The code starts by importing necessary libraries such as Pandas, NumPy, Seaborn, Matplotlib for data manipulation, visualization, and regression modeling.

Loading and Inspecting Data:

Loads the Uber dataset from a CSV file.
Displays the first few rows of the dataset and the basic information using df.head() and df.info() respectively.
Drops irrelevant columns ('Unnamed: 0', 'key').
Handles missing values by filling null values in 'dropoff_latitude' and 'dropoff_longitude' columns with mean and median values, respectively.
Feature Engineering:

Converts the 'pickup_datetime' column to a datetime object.
Extracts features like hour, day, month, year, and day of the week from the 'pickup_datetime' column.
Drops the 'pickup_datetime' column after feature extraction.
Handling Outliers:

Identifies and treats outliers using a function that employs the IQR (Interquartile Range) method.
Generates boxplots before and after outlier treatment to visually check the impact.
Calculating Distance:

Utilizes the Haversine formula to calculate the distance between pickup and dropoff coordinates.
Filters the dataset based on distance criteria and invalid coordinate ranges.
Data Analysis:

Examines the dataset for null values, visualizes their presence using a heatmap, and checks the correlation between variables.
Regression Modeling:

Splits the dataset into features (X) and the target variable (y).
Splits the data into training and testing sets.
Utilizes Linear Regression and Random Forest Regression for modeling.
Evaluates model performance using R-squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).


2. email theory

K-Nearest Neighbor (KNN) Algorithm:
K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning
technique. K-NN algorithm assumes the similarity between the new case/data and available cases and put the
new case into the category that is most similar to the available categories. K-NN algorithm stores all the
available data and classifies a new data point based on the similarity. This means when new data appears then
it can be easily classified into a well suited category by using K- NN algorithm. K-NN is a non-parametric
algorithm, which means it does not make any assumption on underlying data.

How does K-NN work?
The K-NN working can be explained on the basis of the below algorithm:
• Step-1: Select the number K of the neighbors

Department of Computer Engineering, SITS, Narhe LP III Lab Manual
• Step-2: Calculate the Euclidean distance of K number of neighbors
• Step-3: Take the K nearest neighbors as per the calculated Euclidean distance.
• Step-4: Among these k neighbors, count the number of the data points in each category.
• Step-5: Assign the new data points to that category for which the number of the neighbor is
maximum.
• Step-6: Our model is ready.

Advantages of KNN Algorithm:
• It is simple to implement.
• It is robust to the noisy training data
• It can be more effective if the training data is large.
Disadvantages of KNN Algorithm:
• Always needs to determine the value of K which may be complex some time.
• The computation cost is high because of calculating the distance between the data points for all the
training samples.

Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used
for Classification as well as Regression problems. However, primarily, it is used for Classification problems
in Machine Learning.
The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-
dimensional space into classes so that we can easily put the new data point in the correct category in the
future. This best decision boundary is called a hyperplane.
SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are called
as support vectors, and hence algorithm is termed as Support Vector Machine.
SVM algorithm can be used for Face detection, image classification, text categorization, etc.
For analysis of performance of KNN and SVM, use different evaluation metrics like accuracy, precision,
recall, F-score, etc.

**************Code:**************
ibraries Import:

The code imports necessary libraries such as Pandas, NumPy, Seaborn, Matplotlib, and scikit-learn modules.
Data Loading and Initial Inspection:

Reads an 'emails.csv' file into a Pandas DataFrame named 'df'.
Displays the first few rows of the dataset using df.head().
Checks for missing values in the DataFrame using df.isnull().sum() and drops those rows with missing values using df.dropna(inplace=True).
Drops the 'Email No.' column as it seems to be an identifier and might not provide predictive value.
Data Preparation for Modeling:

Separates the data into feature variables (X) and the target variable (y) where 'Prediction' column is the target variable and the rest are features.
Scales the feature variables using scale() from scikit-learn's preprocessing module.
Train-Test Split:

Splits the dataset into training and testing sets using train_test_split() from scikit-learn, with a test size of 30% and a specified random state.
K-Nearest Neighbors (KNN) Classification:

Initiates a KNN classifier with 7 neighbors.
Trains the KNN model using the training data with knn.fit(X_train, y_train).
Performs predictions on the test data using the trained model and stores the results in y_pred.
Calculates and prints the accuracy score of the KNN model using metrics.accuracy_score().
Generates a confusion matrix using metrics.confusion_matrix() to evaluate the performance of the KNN model.
Support Vector Machine (SVM) Classification:

Initializes an SVM model with a regularization parameter 'C' set to 1.
Trains the SVM model using the training data with model.fit(X_train, y_train).
Performs predictions on the test data using the trained SVM model and stores the results in y_pred.
Computes and prints the accuracy score of the SVM model using metrics.accuracy_score().
Output and Evaluation:

Displays the predicted values, accuracy scores for both KNN and SVM models, and confusion matrices for better understanding of model performance.


3. churn modelling theory

Normalization in Machine Learning:
Normalization is one of the most frequently used data preparation techniques, which helps us to change the
values of numeric columns in the dataset to use a common scale. Normalization is a scaling technique in
Machine Learning applied during data preparation to change the values of numeric columns in the dataset to
use a common scale. It is not necessary for all datasets in a model. It is required only when features of
machine learning models have different ranges.
Mathematically, we can calculate normalization with the below formula:
Xn = (X - Xminimum) / ( Xmaximum - Xminimum)
o Xn = Value of Normalization
o Xmaximum = Maximum value of a feature
o Xminimum = Minimum value of a feature

accuracy= no. of correct predictions/total no. of predictions
accuracy = tp +tn/tp + tn + fp + fn

confusion matrix:
The confusion matrix is a matrix used to determine the performance of the classification models for a given
set of test data. It can only be determined if the true values for test data are known.

n = total predictions |    actual: no     |    actual: no
______________________|___________________|________________ 
predicted:no          |   true negative   |  false positive
______________________|___________________|________________               
predicted: yes        |   false negative  |   true positive


True Negative: Model has given prediction No, and the real or actual value was also No.
o True Positive: The model has predicted yes, and the actual value was also true.
o False Negative: The model has predicted no, but the actual value was Yes, it is also called as Type-II
error.
o False Positive: The model has predicted Yes, but the actual value was No. It is also called a Type-I
error.


*****************code:***************

Data Loading and Initial Analysis:
Imports necessary libraries such as Pandas, NumPy, Seaborn, Matplotlib, and the required TensorFlow/Keras modules.
Reads the dataset into a Pandas DataFrame named df.
Displays the first few rows of the DataFrame using df.head().
Checks the shape, missing values, information, and data types of the dataset using functions like df.shape, df.isnull().sum(), df.info(), df.dtypes, and df.columns.
Drops unnecessary columns ('RowNumber', 'Surname', 'CustomerId') from the DataFrame.

Data Visualization:
Defines a function visualization() to create histograms comparing 'Tenure' and 'Age' for customers who exited ('Exited' = 1) and those who did not ('Exited' = 0).
Visualizes the 'Tenure' and 'Age' data using the defined function.

Data Preprocessing:
Prepares feature and target variables (X and y) for the model by selecting specific columns and performing one-hot encoding on categorical variables ('Geography' and 'Gender').
Splits the data into training and testing sets using train_test_split().

Data Normalization:
Scales the numerical feature variables using StandardScaler to standardize them (mean 0 and standard deviation 1).

Building a Neural Network Model:
Creates a sequential neural network model using Keras.

Adds three layers using Dense:
Two hidden layers with ReLU activation and six units each.
An output layer with a Sigmoid activation function.
Compiles the model using 'adam' optimizer and 'binary_crossentropy' loss function.
Displays a summary of the model's architecture.
Fits the model using the training data for 50 epochs with a batch size of 10.

Predictions and Evaluation:
Makes predictions on the test data and rounds the values to binary results using a threshold of 0.5.
Computes the confusion matrix, accuracy score, and generates a classification report to evaluate the model's performance.
Visualizes the confusion matrix using Seaborn's heatmap.

4. k nearest neighbor diabetes  theory

K-Nearest Neighbor (KNN) Algorithm:
K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning
technique. K-NN algorithm assumes the similarity between the new case/data and available cases and put the
new case into the category that is most similar to the available categories. K-NN algorithm stores all the
available data and classifies a new data point based on the similarity. This means when new data appears then
it can be easily classified into a well suited category by using K- NN algorithm. K-NN is a non-parametric
algorithm, which means it does not make any assumption on underlying data.

How does K-NN work?
The K-NN working can be explained on the basis of the below algorithm:
• Step-1: Select the number K of the neighbors
• Step-2: Calculate the Euclidean distance of K number of neighbors
• Step-3: Take the K nearest neighbors as per the calculated Euclidean distance.
• Step-4: Among these k neighbors, count the number of the data points in each category.
• Step-5: Assign the new data points to that category for which the number of the neighbor is
maximum.
• Step-6: Our model is ready.

Accuracy - Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted
observation to the total observations. One may think that, if we have high accuracy then our model is best.
Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive
and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the
performance of your model. For our model, we have got 0.803 which means our model is approx. 80%
accurate.

Accuracy = TP+TN/TP+FP+FN+TN

Precision - Precision is the ratio of correctly predicted positive observations to the total predicted positive
observations. The question that this metric answer is of all passengers that labeled as survived, how many
actually survived? High precision relates to the low false positive rate. We have got 0.788 precision which is
pretty good.

Precision = TP/TP+FP

Recall (Sensitivity) - Recall is the ratio of correctly predicted positive observations to the all observations in
actual class - yes. The question recall answers is: Of all the passengers that truly survived, how many did we
label? We have got recall of 0.631 which is good for this model as it‘s above 0.5.

Recall = TP/TP+FN

Error Rate - what percentage of our prediction are wrong.
Error Rate = 1-Accuracy

accuracy= no. of correct predictions/total no. of predictions
accuracy = tp +tn/tp + tn + fp + fn

confusion matrix:
The confusion matrix is a matrix used to determine the performance of the classification models for a given
set of test data. It can only be determined if the true values for test data are known.

n = total predictions |    actual: no     |    actual: no
______________________|___________________|________________ 
predicted:no          |   true negative   |  false positive
______________________|___________________|________________               
predicted: yes        |   false negative  |   true positive


True Negative: Model has given prediction No, and the real or actual value was also No.
o True Positive: The model has predicted yes, and the actual value was also true.
o False Negative: The model has predicted no, but the actual value was Yes, it is also called as Type-II
error.
o False Positive: The model has predicted Yes, but the actual value was No. It is also called a Type-I
error.


*******code:**********

Libraries and Data Loading:
Libraries like Pandas, NumPy, Seaborn, Matplotlib are imported, and warnings are suppressed for cleaner output.
The 'diabetes.csv' file is read into a Pandas DataFrame named 'df'.

Data Preparation:
The column names in the DataFrame are checked using df.columns.
Null values are examined across columns using df.isnull().sum().

Data Splitting and Scaling:
The dataset is prepared for modeling. Features (X) exclude the 'Outcome' column, which represents the target variable 'y' (Outcome of diabetes).
Features (X) are scaled using scale() from scikit-learn to standardize the data.

Train-Test Split:
The dataset is split into training and testing sets using train_test_split() from scikit-learn. It uses a test size of 30% and a specified random state.

K-Nearest Neighbors Classification:
A KNN classifier is instantiated with 7 neighbors using KNeighborsClassifier.
The model is trained on the training data using knn.fit(X_train, y_train).
Predictions are made on the test data using the trained model (knn.predict(X_test)).

Model Evaluation Metrics:
A variety of classification metrics are calculated and printed to evaluate the model's performance.
Confusion Matrix: Displays true positives, true negatives, false positives, and false negatives.
Accuracy Score: Measures the overall accuracy of the model's predictions.

Error Rate: Measures the rate of misclassified examples.
Precision: Measures the ratio of correctly predicted positive observations to the total predicted positives.
Recall: Measures the ratio of correctly predicted positive observations to the actual positives.
Classification Report: Provides a comprehensive report with precision, recall, F1-score, and support for each class.


5. k means clustering sales_data_sameple theory

K-Means Clustering Algorithm:
K-Means Clustering is an Unsupervised Learning algorithm, which groups the unlabeled dataset into different
clusters. Here K defines the number of pre-defined clusters that need to be created in the process, as if K=2,
there will be two clusters, and for K=3, there will be three clusters, and so on.
It is an iterative algorithm that divides the unlabeled dataset into k different clusters in such a way that each
dataset belongs only one group that has similar properties.
It allows us to cluster the data into different groups and a convenient way to discover the categories of groups
in the unlabeled dataset on its own without the need for any training.
It is a centroid-based algorithm, where each cluster is associated with a centroid. The main aim of this
algorithm is to minimize the sum of distances between the data point and their corresponding clusters.
The algorithm takes the unlabeled dataset as input, divides the dataset into k-number of clusters, and repeats
the process until it does not find the best clusters. The value of k should be predetermined in this algorithm.
The k-means clustering algorithm mainly performs two tasks:
o Determines the best value for K center points or centroids by an iterative process.

o Assigns each data point to its closest k-center. Those data points which are near to the particular k-
center, create a cluster.

How does the K-Means Algorithm Work?
The working of the K-Means algorithm is explained in the below steps:

Department of Computer Engineering, SITS, Narhe LP III Lab Manual
Step-1: Select the number K to decide the number of clusters.
Step-2: Select random K points or centroids. (It can be other from the input dataset).
Step-3: Assign each data point to their closest centroid, which will form the predefined K clusters.
Step-4: Calculate the variance and place a new centroid of each cluster.
Step-5: Repeat the third step, which means reassign each datapoint to the new closest centroid of each cluster.
Step-6: If any reassignment occurs, then go to step-4 else go to FINISH.
Step-7: The model is ready.
Elbow Method:
The Elbow method is one of the most popular ways to find the optimal number of clusters. This method uses
the concept of WCSS value. WCSS stands for Within Cluster Sum of Squares, which defines the total
variations within a cluster. The formula to calculate the value of WCSS (for 3 clusters) is given below:
WCSS= ∑Pi in Cluster1 distance(Pi C1)^2+∑Pi in Cluster2distance(Pi C2)^2+∑Pi in CLuster3 distance(Pi C3)^2


**********code:**********

Libraries and Data Loading:
Import of necessary libraries: Pandas, NumPy, Seaborn, Matplotlib, and specific classes/modules from scikit-learn.
Reads the dataset into a Pandas DataFrame named df.
Displays the first few rows of the DataFrame using df.head().
Investigates the shape, summary statistics, information, missing values, and data types using methods like df.shape, df.describe(), df.info(), df.isnull().sum(), and df.dtypes.

Data Preprocessing:
Removes various categorical and null-containing columns that are deemed unnecessary for the analysis.
Converts categorical columns ('PRODUCTLINE' and 'DEALSIZE') into one-hot encoded dummy variables.
Converts the 'PRODUCTCODE' column to categorical codes.
Drops the 'ORDERDATE' column as 'Month' is already included.
Verifies that all data types are converted into numeric.

Elbow Method and KMeans Clustering:
Determines the optimal number of clusters by using the Elbow Method. Plots the distortion against a range of different cluster numbers.
Constructs the KMeans model with the chosen number of clusters (in this case, 3) and fits it to the data.
Predicts the clusters for each data point and counts the number of samples in each cluster.
Utilizes Principal Component Analysis (PCA) to reduce the dimensions of the dataset to two components for visualization.

Data Visualization:
Displays a scatter plot of the data points in the reduced two-dimensional space.
Identifies and plots the cluster centroids obtained from the KMeans model in the reduced space.
Separately visualizes the clusters using different colors for each cluster.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%